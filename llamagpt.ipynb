{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzfprKWbZ6TKCUj53Xej7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthik0510/Karthik0510/blob/main/llamagpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "id": "Qqw9ciRxSour"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "yZrJeFJxSqJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename, cache_dir=CACHE_DIR)"
      ],
      "metadata": {
        "id": "xafz04bTU_q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""
      ],
      "metadata": {
        "id": "1jk0qWYVSsEw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "CACHE_DIR = \"/content/drive/MyDrive/Colab Notebooks/cache\""
      ],
      "metadata": {
        "id": "X4c01YfiSufc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f\"{CACHE_DIR}/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/{model_basename}\""
      ],
      "metadata": {
        "id": "qVsGc4OpSwFj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import OutputFixingParser, PydanticOutputParser\n",
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "tzBbOW59XyPd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0,openai_api_key=\"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\")"
      ],
      "metadata": {
        "id": "tDMAmpcTW67S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096, # Context window\n",
        ")"
      ],
      "metadata": {
        "id": "knY5fCAnUkjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AAIirYSJVMFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "STQbztrnWPjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/Resumes'\n",
        "import os\n",
        "import PyPDF2\n",
        "\n",
        "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "resumes=[]\n",
        "for pdf_file in pdf_files:\n",
        "    pandr=[]\n",
        "    full_path = os.path.join(folder_path, pdf_file)\n",
        "    pdfFileObj = open(full_path, 'rb')\n",
        "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
        "    pages=\"\"\n",
        "    for i in range(len(pdfReader.pages)):\n",
        "      pageObj = pdfReader.pages[i]\n",
        "      pages=pages+pageObj.extract_text()\n",
        "    print(pages)\n",
        "    pdfFileObj.close()\n",
        "    pandr.append(full_path)\n",
        "    pandr.append(pages)\n",
        "    resumes.append(pandr)"
      ],
      "metadata": {
        "id": "8C1hL36rVZfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=''' '''\n",
        "format_instructions= '''{\n",
        "                \"name\": \"Candidate Name\",\n",
        "                \"gmail\": \"Candidate gmail\",\n",
        "                \"phone\": \"Candidate Phone\",\n",
        "                \"education\": [\n",
        "                    {\n",
        "                        \"college\": \"College Name\",\n",
        "                        \"degree\": \"Degree Name\",\n",
        "                        \"start_date\": \"Start Date\",\n",
        "                        \"end_date\": \"End Date\"\n",
        "                    }\n",
        "                ],\n",
        "                \"experiences\": [\n",
        "                    {\n",
        "                        \"company\": \"Company Name\",\n",
        "                        \"title\": \"Job Title\",\n",
        "                        \"start_date\": \"Start Date\",\n",
        "                        \"end_date\": \"End Date\",\n",
        "                        \"duration\": \"Job Duration\",\n",
        "                        \"location\": \"Location\"\n",
        "                    }\n",
        "                ]\n",
        "            }'''\n",
        "\n",
        "prompt = f'''\n",
        "Extract only the instructed information from the candidate's  CV and give an output it in the specified JSON  format.\n",
        "                        Do not include any additional entries.:\n",
        "                        {format_instructions}.\n",
        "                        And Candidate CV:\n",
        "                        {text}\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "TEtTOVTVWQkz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = PromptTemplate(\n",
        "            template=\"\"\"Extract only the instructed information from the candidate's LinkedIn CV and output it in the specified JSON format.\n",
        "                        Do not include any additional entries.:\n",
        "                        {format_instructions}.\n",
        "                        And Candidate CV:\n",
        "                        {cv_text}\n",
        "                        \"\"\",\n",
        "            input_variables=[\"cv_text\"],\n",
        "            partial_variables={\n",
        "                \"format_instructions\": format_instructions,\n",
        "            },\n",
        "            validate_template=False,\n",
        ")"
      ],
      "metadata": {
        "id": "ZFpOgW02ZATb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json(s):\n",
        "  for i in range(len(s)):\n",
        "    if s[i]==\"{\":\n",
        "      o=i\n",
        "      break\n",
        "  for i in range(len(s)):\n",
        "    if s[i]==\"}\":\n",
        "      c=i\n",
        "  return s[o:c+1]"
      ],
      "metadata": {
        "id": "LgobQ-1KWStW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "7-f49-NrWWja"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_tokens(lcpp_llm, prompt):\n",
        "  return len(lcpp_llm.tokenize(prompt.encode(\"utf-8\")))\n",
        "\n",
        "get_num_tokens(lcpp_llm, prompt)"
      ],
      "metadata": {
        "id": "SYnSseAqUwx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class OpenAILinkedinEducation(BaseModel):\n",
        "    college: Optional[str] = Field(\n",
        "        None, title=\"Name of the college or institute\"\n",
        "    )\n",
        "    degree: Optional[str] = Field(None, title=\"Name of the degree\")\n",
        "    start_date: Optional[str] = Field(None, title=\"Start date\")\n",
        "    end_date: Optional[str] = Field(None, title=\"End date\")\n",
        "\n",
        "\n",
        "class OpenAILinkedinExperience(BaseModel):\n",
        "    company: Optional[str] = Field(None, title=\"Name of the company\")\n",
        "    title: Optional[str] = Field(None, title=\"Job title\")\n",
        "    start_date: Optional[str] = Field(None, title=\"Start date\")\n",
        "    end_date: Optional[str] = Field(None, title=\"End date\")\n",
        "    duration: Optional[str] = Field(None, title=\"Duration\")\n",
        "    location: Optional[str] = Field(None, title=\"Location of the company\")\n",
        "\n",
        "\n",
        "class OpenAILinkedinModel(BaseModel):\n",
        "    name:Optional[str] = Field(None, title=\"Name of the Candidate\")\n",
        "    email: Optional[str] = Field(None, title=\"Email address\")\n",
        "    phone: Optional[str] = Field(None, title=\"Phone number\")\n",
        "    education: List[OpenAILinkedinEducation] = Field(\n",
        "        [], title=\"List of education details\"\n",
        "    )\n",
        "    experiences: List[OpenAILinkedinExperience] = Field(\n",
        "        [], title=\"List of work experiences\"\n",
        "    )\n",
        "    # preferred_location: Optional[str] = Field(None, title=\"Preferred location\")\n",
        "    # current_location: Optional[str] = Field(None, title=\"Current location\")\n",
        "    # linkedin_url: Optional[str] = Field(None, title=\"LinkedIn profile URL\")\n",
        "parser = PydanticOutputParser(pydantic_object=OpenAILinkedinModel)\n"
      ],
      "metadata": {
        "id": "c53-DncUWrUu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IOXfhQuwaFr0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "wkXz93XSaq-S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resumes[0][0]"
      ],
      "metadata": {
        "id": "eQcjIjjncecG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\"model\":[\"model\"],\n",
        "                \"link\":[\"link\"],\n",
        "                \"time\":[\"time\"],\n",
        "                \"cost\":[\"cost\"],\n",
        "            'name': [\"name\"],\n",
        "            'gmail': [\"gmail\"],\n",
        "            'phone':[\"phone number\"],\n",
        "            'education':[\"education \"],\n",
        "            'experiences':[\"experiences\"]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "        # Path to the existing CSV file\n",
        "csv_file_path = 'resumes.csv'\n",
        "\n",
        "        # Append the DataFrame to the existing CSV file\n",
        "df.to_csv(csv_file_path, mode='a', header=False, index=False)"
      ],
      "metadata": {
        "id": "zS8tiUuZdI49"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt code\n",
        "for i in  resumes:\n",
        "  with get_openai_callback() as cb:\n",
        "    try:\n",
        "        start_time=time.time()\n",
        "        output = llm.invoke(template.format(cv_text=i[1]))\n",
        "        json_parser = SimpleJsonOutputParser()\n",
        "        res = json_parser.parse(output.content)\n",
        "        print(res)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        data = {\"model\":[],\n",
        "                \"link\":[],\n",
        "                \"time\":[],\n",
        "                \"cost\":[],\n",
        "            'name': [],\n",
        "            'gmail': [],\n",
        "            'phone':[],\n",
        "            'education':[],\n",
        "            'experiences':[]}\n",
        "        for j in res:\n",
        "          data[j].append(res[j])\n",
        "        data[\"model\"].append(llm.model_name)\n",
        "        data[\"link\"].append(i[0])\n",
        "\n",
        "        data[\"time\"].append(elapsed_time)\n",
        "        data[\"cost\"].append(cb.total_cost)\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Path to the existing CSV file\n",
        "        csv_file_path = 'resumes.csv'\n",
        "\n",
        "        # Append the DataFrame to the existing CSV file\n",
        "        df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
        "    except Exception as e:\n",
        "          print(f\"Error processing entry: {e}\")\n",
        "          continue"
      ],
      "metadata": {
        "id": "GdZGQEjeewkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llama 2 code\n",
        "for i in resumes :\n",
        "    try :\n",
        "      text=i[1]\n",
        "      prompt = f'''\n",
        "Extract only the instructed information from the candidate's  CV and give an output it in the specified JSON  format.\n",
        "                        Do not include any additional entries.:\n",
        "                        {format_instructions}.\n",
        "                        And Candidate CV:\n",
        "                        {text}'''\n",
        "\n",
        "      start_time=time.time()\n",
        "\n",
        "      response = lcpp_llm(\n",
        "      prompt=prompt,\n",
        "      max_tokens=1024,\n",
        "      temperature=0.5,\n",
        "      top_p=0.95,\n",
        "      repeat_penalty=1.2,\n",
        "      top_k=50,\n",
        "      stop = ['USER:'],\n",
        "      echo=False\n",
        "      )\n",
        "\n",
        "      end_time = time.time()\n",
        "      elapsed_time = end_time - start_time\n",
        "\n",
        "      gj=get_json(response[\"choices\"][0][\"text\"])\n",
        "      res = json.loads(gj)\n",
        "      data = {\"model\":[],\n",
        "                \"link\":[],\n",
        "                \"time\":[],\n",
        "                \"cost\":[],\n",
        "            'name': [],\n",
        "            'gmail': [],\n",
        "            'phone':[],\n",
        "            'education':[],\n",
        "            'experiences':[]}\n",
        "\n",
        "      for j in res:\n",
        "        data[j].append(res[j])\n",
        "      data[\"model\"].append(\"llama2\")\n",
        "      data[\"link\"].append(i[0])\n",
        "      data[\"time\"].append(elapsed_time)\n",
        "      data[\"cost\"].append(cb.total_cost)\n",
        "      df = pd.DataFrame(data)\n",
        "      csv_file_path = 'resumes.csv'\n",
        "      df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing entry: {e}\")\n",
        "        continue"
      ],
      "metadata": {
        "id": "0ihec81ce1OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT AND LLAMA 2 CODE COMBINED\n",
        "for i in  resumes:\n",
        "  with get_openai_callback() as cb:\n",
        "    try:\n",
        "        start_time=time.time()\n",
        "        output = llm.invoke(template.format(cv_text=i[1]))\n",
        "        json_parser = SimpleJsonOutputParser()\n",
        "        res = json_parser.parse(output.content)\n",
        "        print(res)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        data = {\"model\":[],\n",
        "                \"link\":[],\n",
        "                \"time\":[],\n",
        "                \"cost\":[],\n",
        "            'name': [],\n",
        "            'gmail': [],\n",
        "            'phone':[],\n",
        "            'education':[],\n",
        "            'experiences':[]}\n",
        "        for j in res:\n",
        "          data[j].append(res[j])\n",
        "        data[\"model\"].append(llm.model_name)\n",
        "        data[\"link\"].append(i[0])\n",
        "\n",
        "        data[\"time\"].append(elapsed_time)\n",
        "        data[\"cost\"].append(cb.total_cost)\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Path to the existing CSV file\n",
        "        csv_file_path = 'resumes.csv'\n",
        "\n",
        "        # Append the DataFrame to the existing CSV file\n",
        "        df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
        "    except Exception as e:\n",
        "          print(f\"Error processing entry: {e}\")\n",
        "          continue\n",
        "\n",
        "      #LLAMA 2 CODE IS HERE\n",
        "\n",
        "#   try :\n",
        "#       text=i[1]\n",
        "#       prompt = f'''\n",
        "# Extract only the instructed information from the candidate's  CV and give an output it in the specified JSON  format.\n",
        "#                         Do not include any additional entries.:\n",
        "#                         {format_instructions}.\n",
        "#                         And Candidate CV:\n",
        "#                         {text}'''\n",
        "\n",
        "#       start_time=time.time()\n",
        "\n",
        "#       response = lcpp_llm(\n",
        "#       prompt=prompt,\n",
        "#       max_tokens=1024,\n",
        "#       temperature=0.5,\n",
        "#       top_p=0.95,\n",
        "#       repeat_penalty=1.2,\n",
        "#       top_k=50,\n",
        "#       stop = ['USER:'],\n",
        "#       echo=False\n",
        "#       )\n",
        "\n",
        "#       end_time = time.time()\n",
        "#       elapsed_time = end_time - start_time\n",
        "\n",
        "#       gj=get_json(response[\"choices\"][0][\"text\"])\n",
        "#       res = json.loads(gj)\n",
        "#       data = {\"model\":[],\n",
        "#                 \"link\":[],\n",
        "#                 \"time\":[],\n",
        "#                 \"cost\":[],\n",
        "#             'name': [],\n",
        "#             'gmail': [],\n",
        "#             'phone':[],\n",
        "#             'education':[],\n",
        "#             'experiences':[]}\n",
        "\n",
        "#       for j in res:\n",
        "#         data[j].append(res[j])\n",
        "#       data[\"model\"].append(\"llama2\")\n",
        "#       data[\"link\"].append(i[0])\n",
        "#       data[\"time\"].append(elapsed_time)\n",
        "#       data[\"cost\"].append(cb.total_cost)\n",
        "#       df = pd.DataFrame(data)\n",
        "#       csv_file_path = 'resumes.csv'\n",
        "#       df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
        "#   except Exception as e:\n",
        "#         print(f\"Error processing entry: {e}\")\n",
        "#         continue"
      ],
      "metadata": {
        "id": "ycfxgNLtWbpr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}