{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthik0510/Karthik0510/blob/main/lda_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcTvHYaEsgWr"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf==3.14.0\n",
        "!pip install tiktoken==0.4.0\n",
        "!pip install langchain==0.0.353\n",
        "!pip install openai==0.27.8\n",
        "!pip install gdown==4.7.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from pypdf import PdfReader\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "P3QIHJ48sipR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text, stop_words):\n",
        "    \"\"\"\n",
        "    Tokenizes and preprocesses the input text, removing stopwords and short\n",
        "    tokens.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to preprocess.\n",
        "        stop_words (set): A set of stopwords to be removed from the text.\n",
        "    Returns:\n",
        "        list: A list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for token in simple_preprocess(text, deacc=True):\n",
        "        if token not in stop_words and len(token) > 3:\n",
        "            result.append(token)\n",
        "    return result"
      ],
      "metadata": {
        "id": "aN77qXQKswIV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_lists_from_text(file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Extracts topics and their associated words from a text document using the\n",
        "    Latent Dirichlet Allocation (LDA) algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        file (str): The path to the text file for topic extraction.\n",
        "        num_topics (int): The number of topics to discover.\n",
        "        words_per_topic (int): The number of words to include per topic.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of num_topics sublists, each containing relevant words\n",
        "        for a topic.\n",
        "    \"\"\"\n",
        "    # Load the text file\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        documents = [f.read()]\n",
        "    print(documents)\n",
        "\n",
        "    if len(documents)<300:\n",
        "      num_topics=3\n",
        "    elif len(documents)>=300 and len(documents)<1000:\n",
        "      num_topics=5\n",
        "    else:\n",
        "      num_topics=8\n",
        "    # Preprocess the documents\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words(['english', 'spanish']))\n",
        "    processed_documents = [preprocess(doc,stop_words) for doc in documents]\n",
        "\n",
        "    # Create a dictionary and a corpus\n",
        "    dictionary = corpora.Dictionary(processed_documents)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(\n",
        "        corpus,\n",
        "        num_topics=num_topics,\n",
        "        id2word=dictionary,\n",
        "        passes=15\n",
        "    )\n",
        "\n",
        "    # Retrieve the topics and their corresponding words\n",
        "    topics = lda_model.print_topics(num_words=words_per_topic)\n",
        "\n",
        "    # Store each list of words from each topic into a list\n",
        "    topics_ls = []\n",
        "    for topic in topics:\n",
        "        words = topic[1].split(\"+\")\n",
        "        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
        "        topics_ls.append(topic_words)\n",
        "\n",
        "    return topics_ls"
      ],
      "metadata": {
        "id": "5CdkEYJ8sz31"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topics_from_txt(llm, file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Generates descriptive prompts for LLM based on topic words extracted from a\n",
        "    PDF document.\n",
        "\n",
        "    This function takes the output of `get_topic_lists_from_pdf` function,\n",
        "    which consists of a list of topic-related words for each topic, and\n",
        "    generates an output string in bulleted nested list format.\n",
        "\n",
        "    Parameters:\n",
        "        llm (LLM): An instance of the Large Language Model (LLM) for generating\n",
        "        responses.\n",
        "        file (str): The path to the PDF file for extracting topic-related words.\n",
        "        num_topics (int): The number of topics to consider.\n",
        "        words_per_topic (int): The number of words per topic to include.\n",
        "\n",
        "    Returns:\n",
        "        str: A response generated by the language model based on the provided\n",
        "        topic words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract topics and convert them to string\n",
        "    list_of_topicwords = get_topic_lists_from_text(file, num_topics,\n",
        "                                                  words_per_topic)\n",
        "    string_lda = \"\"\n",
        "    for list in list_of_topicwords:\n",
        "        string_lda += str(list) + \"\\n\"\n",
        "\n",
        "    # Create the template\n",
        "    template_string = '''Generate topic in few words in a sentence of the {num_topics}\n",
        "        double-quote delimited lists in a simple sentence. The lists are the result of an\n",
        "        algorithm for topic discovery.\n",
        "\n",
        "\n",
        "\n",
        "        Do not provide an introduction or a conclusion, only Generate the\n",
        "        topics. Don't mention the word \"topic\" or anything similar to \"the first stopic is about...\" when describing the topics.\n",
        "\n",
        "\n",
        "        Desired example output:\n",
        "        1: Bearish Trading Indicators\n",
        "        2: Crypto Market Trends\n",
        "\n",
        "        1: Borussia Dortmund in the Champions League final\n",
        "        2: Dani Olmo scores for RB Leipzig against Atletico Madrid\n",
        "\n",
        "\n",
        "        Lists: \"\"\"{string_lda}\"\"\"\n",
        "         '''\n",
        "\n",
        "    # LLM call\n",
        "    prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    response = chain.run({\n",
        "        \"string_lda\" : string_lda,\n",
        "        \"num_topics\" : num_topics\n",
        "        })\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "M7CyPxzns-U3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key = \"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\"\n",
        "llm = OpenAI(openai_api_key=openai_key, max_tokens=-1)"
      ],
      "metadata": {
        "id": "pBIahIIUtB2e",
        "outputId": "4b78d3a8-9b24-48b3-b741-4d7a4376f33a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#send a zip file over here to get the output\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import tempfile\n",
        "import glob\n",
        "\n",
        "num_topics = 5\n",
        "words_per_topic = 10\n",
        "# Specify the path to your .zip file\n",
        "zip_file_path = \"/content/testing_topic.zip\"\n",
        "\n",
        "# Create a temporary directory to extract the files\n",
        "with tempfile.TemporaryDirectory() as tempdir:\n",
        "    # Extract the .zip file to the temporary directory\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(tempdir)\n",
        "\n",
        "    # Use glob to find all .txt and .pdf files in the temporary directory\n",
        "    files = glob.glob(f\"{tempdir}/*.txt\") + glob.glob(f\"{tempdir}/*.pdf\")\n",
        "\n",
        "    l = []\n",
        "    for file_path in files:\n",
        "        # Determine the file type and call the appropriate function\n",
        "        if file_path.lower().endswith('.txt'):\n",
        "            summary = topics_from_txt(llm, file_path, num_topics, words_per_topic)\n",
        "        elif file_path.lower().endswith('.pdf'):\n",
        "            summary = topics_from_pdf(llm, file_path, num_topics, words_per_topic)\n",
        "        l.append([file_path,summary])"
      ],
      "metadata": {
        "id": "TNYFRFoStHFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promptlist=[]\n",
        "for x in l:\n",
        "  prompt=''''''\n",
        "  prompt+=x[0]+\":\"\n",
        "  for i in range(1,len(x)):\n",
        "    prompt+=x[i]\n",
        "    prompt+=\"\\n\\n\"\n",
        "  promptlist.append(prompt)\n",
        "  #promptlist is a list of headlines and also the file path which is used to map files and also categorize them using llm\n",
        "for i in promptlist:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtyK5OlMtJRK",
        "outputId": "b5b5eb6d-3a90-4624-9633-f9aa0e38beca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmp0jstlf1y/news_2.txt:\n",
            "1: Mumbai Firing Incident\n",
            "2: Salman Khan's Involvement\n",
            "3: Police Response in the Village\n",
            "4: District Administration's Actions\n",
            "5: Night of Chaos in Mumbai's District\n",
            "\n",
            "\n",
            "/tmp/tmp0jstlf1y/news_3.txt:\n",
            "1: Indian Political Leaders\n",
            "2: Election Candidates\n",
            "3: Personal Sacrifices\n",
            "\n",
            "\n",
            "/tmp/tmp0jstlf1y/crypto_1.txt:\n",
            "1: Crypto Regulations and Wildlife Conservation\n",
            "2: Jail Time for Crypto Scammers and Chairman Rostin's Challenge\n",
            "3: Warren's Full Accounting Collapse and Government Interactions with Crypto Markets\n",
            "\n",
            "\n",
            "/tmp/tmp0jstlf1y/news6.txt:\n",
            "1: Payment Aggregators and Merchant Networth\n",
            "2: Bank Activity Authorisation and Due Diligence\n",
            "\n",
            "\n",
            "/tmp/tmp0jstlf1y/news4.txt:\n",
            "1: Durga Puja violence in West Bengal\n",
            "2: Chief Minister Mamata Banerjee under attack\n",
            "\n",
            "\n",
            "/tmp/tmp0jstlf1y/news3.txt:\n",
            "1: Salman Khan's Involvement in Mumbai Firing Incident\n",
            "2: Police Investigation into Crime Branch and Bishnoi's Role\n",
            "\n",
            "\n",
            "/tmp/tmp0jstlf1y/news5.txt:1: Sabha Election Phase\n",
            "         2: Modi in Bengal\n",
            "         3: Congress Party Strategies\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "openai_key = \"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\"\n",
        "llm = OpenAI(openai_api_key=openai_key, max_tokens=-1)\n",
        "list_of_headings= promptlist\n",
        "\n",
        "\n",
        "example='''{[Machine Learning:\n",
        "    [filepath1234],\n",
        "\n",
        "\n",
        "Artificial Intelligence:\n",
        "[\n",
        "    filepath898,\n",
        "    filepath345],\n",
        "\n",
        "Data Science:\n",
        "    [filepath345,\n",
        "    filepath1234]}'''\n",
        "\n",
        "\n",
        "prompt=f'''You excel in the art of categorizing diverse topics and news items effectively. Your task is to categorize\n",
        "a set of main headings into distinct groups, assigning a name to each category. Your goal is to streamline the topics into FEWER,\n",
        " yet significant categories and output them in a dictionary format . Below are a few main headings for you to work with. Dive in and create an optimal categorization!\n",
        "->{list_of_headings}\n",
        "Do not chaange the filepath assinged to the headlines and only print the filepath under the topic do not print the sentence\n",
        "give the output in the format of a dictionary\n",
        "example output->\n",
        "{example}\n",
        " '''\n",
        "\n",
        "\n",
        "output = llm.invoke(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "QCarOSYsteiP",
        "outputId": "203500cd-7890-4c8e-8d2e-3fd5a137de11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{News:\n",
            "    [\n",
            "        /tmp/tmp0jstlf1y/news_2.txt,\n",
            "        /tmp/tmp0jstlf1y/news_3.txt,\n",
            "        /tmp/tmp0jstlf1y/news4.txt,\n",
            "        /tmp/tmp0jstlf1y/news3.txt,\n",
            "        /tmp/tmp0jstlf1y/news5.txt\n",
            "    ],\n",
            "\n",
            "Crypto:\n",
            "    [\n",
            "        /tmp/tmp0jstlf1y/crypto_1.txt\n",
            "    ],\n",
            "\n",
            "Politics:\n",
            "    [\n",
            "        /tmp/tmp0jstlf1y/news3.txt\n",
            "    ],\n",
            "\n",
            "Payment and Banking:\n",
            "    [\n",
            "        /tmp/tmp0jstlf1y/news6.txt\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}